import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
import accelerate

# Insert test abstract and input test metric
# then insert series to test and metric

# Check if CUDA is available, then set the default device
if torch.cuda.is_available():
    torch.set_default_device("cuda")
else:
    torch.set_default_device("cpu")

# Load the Orca-2-13b model with the correct device map
model = AutoModelForCausalLM.from_pretrained("microsoft/Orca-2-7b", device_map='auto')

# Load the tokenizer (using the slow tokenizer as recommended)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Orca-2-7b", use_fast=False)

# Define system and user messages
system_message = "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior."

user_message = "Can you give me 5 complex terms from this sentence? '''Experiment simulation result express: the result of immune genetic algorithm is better than traditional genetic algorithm in the circumstance of the same clusters and the same evolution generation.'''' "

prompt = f"system\n{system_message}\nuser\n{user_message}\nassistant"

# Tokenize the input prompt
inputs = tokenizer(prompt, return_tensors='pt')

# Generate the model output
output_ids = model.generate(inputs["input_ids"])
answer = tokenizer.batch_decode(output_ids)[0]

# Print the first answer
print(answer)

# Second turn message
second_turn_user_message = "Give me the definitions of those words."

# Create the second message
second_turn_message = f"\nuser\n{second_turn_user_message}\nassistant"
second_turn_tokens = tokenizer(second_turn_message, return_tensors='pt', add_special_tokens=False)

# Concatenate the first output with the second turn tokens
second_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)

# Generate the response for the second turn
output_ids_2 = model.generate(second_turn_input)
second_turn_answer = tokenizer.batch_decode(output_ids_2)[0]

# Print the second turn answer
print(second_turn_answer)
